---
layout: post
title: 02_RT系列详解
categories: [algorithm]
description: 
keywords: 
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

# RT-1 与 RT-2 详解

## 1. RT系列概述

**RT (Robotics Transformer)** 是Google开发的专门用于机器人控制的Transformer模型系列。

| 模型 | 发布时间 | 核心创新 | 训练数据规模 |
|------|---------|---------|------------|
| **RT-1** | 2022.12 | Transformer用于机器人控制 | 130K episodes |
| **RT-2** | 2023.07 | 整合VLM的网络知识 | 数百万网络图文对 |

---

## 2. RT-1 架构详解

### 2.1 整体架构图

```
┌────────────────────────────────────────────────────────────────┐
│                     RT-1 Model Architecture                     │
└────────────────────────────────────────────────────────────────┘

输入部分:
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│   Image     │  │  Language   │  │Robot State  │
│ 6 images    │  │ Instruction │  │ (7-DoF pos) │
│ 300x300x3   │  │             │  │             │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       ▼                ▼                ▼
┌──────────────────────────────────────────────────┐
│              EfficientNet-B3 (Shared)            │
│  处理6张图像 (不同视角/时间步)                    │
│                                                  │
│  Image → Tokens: [6, 300, 300, 3]               │
│                ↓                                 │
│         [6, 9, 9, 512]                           │
│                ↓                                 │
│         flatten → [6×81, 512]                    │
└──────────────────┬───────────────────────────────┘
                   │
       ┌───────────┴───────────┐
       ▼                       ▼
┌──────────────┐      ┌────────────────┐
│ Language     │      │  State Tokens  │
│ Tokens       │      │  [1, 512]      │
│ USE Encoder  │      │                │
│ [512]        │      │                │
└──────┬───────┘      └────────┬───────┘
       │                       │
       └───────────┬───────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│          Token Fusion (Concatenation)            │
│                                                  │
│  Combined Tokens = [Image(486) + Lang(1) +      │
│                     State(1)] = 488 tokens       │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│        Transformer Encoder (8 Layers)            │
│                                                  │
│  Layer Structure:                                │
│    ┌─────────────────────────────┐              │
│    │  Multi-Head Self-Attention  │              │
│    │  (8 heads, dim=512)         │              │
│    └────────────┬────────────────┘              │
│                 ▼                                │
│    ┌─────────────────────────────┐              │
│    │    Layer Normalization      │              │
│    └────────────┬────────────────┘              │
│                 ▼                                │
│    ┌─────────────────────────────┐              │
│    │    Feed-Forward Network     │              │
│    │    (MLP: 512 → 2048 → 512)  │              │
│    └────────────┬────────────────┘              │
│                 ▼                                │
│    ┌─────────────────────────────┐              │
│    │    Layer Normalization      │              │
│    └─────────────────────────────┘              │
│                                                  │
│  Output: [488, 512]                              │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│         TokenLearner (Dimension Reduction)       │
│                                                  │
│  488 tokens → 8 learned tokens                   │
│  目的: 减少计算量，提取关键信息                   │
│                                                  │
│  实现: Attention pooling                         │
│    weight = softmax(MLP(tokens))                 │
│    output = weight × tokens                      │
│                                                  │
│  Output: [8, 512]                                │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│      Transformer Decoder (1 Layer)               │
│                                                  │
│  Causal Self-Attention (自回归生成)               │
│                                                  │
│  Output: [8, 512]                                │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│           Action Tokenization Head               │
│                                                  │
│  将动作空间离散化为256个bins:                     │
│                                                  │
│  ┌─────────────────────────────────┐            │
│  │  x: [x_min, x_max] → 256 bins  │            │
│  │  y: [y_min, y_max] → 256 bins  │            │
│  │  z: [z_min, z_max] → 256 bins  │            │
│  │  roll: [-π, π] → 256 bins      │            │
│  │  pitch: [-π, π] → 256 bins     │            │
│  │  yaw: [-π, π] → 256 bins       │            │
│  │  gripper: [0, 1] → 2 classes   │            │
│  └─────────────────────────────────┘            │
│                                                  │
│  7个分类头 (每个256 or 2类)                       │
└──────────────────┬───────────────────────────────┘
                   ▼

输出:
┌──────────────────────────────────────────────────┐
│         Discrete Action Tokens (7维)             │
│                                                  │
│  [x_bin, y_bin, z_bin, roll_bin, pitch_bin,     │
│   yaw_bin, gripper_bin]                          │
│                                                  │
│  Shape: [7] (每个元素是0-255的整数)               │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│        De-tokenization (连续化)                   │
│                                                  │
│  bin_id → continuous value:                      │
│  value = min + (bin_id / 256) * (max - min)      │
│                                                  │
│  Output: [x, y, z, roll, pitch, yaw, gripper]    │
└──────────────────────────────────────────────────┘
```

---

### 2.2 关键设计决策

#### (1) 为什么用6张图像？
```
相机配置:
┌─────────────────────────────────────┐
│      Robot Workspace Views          │
├─────────────────────────────────────┤
│  Camera 1: 头顶俯视 (overhead)       │
│  Camera 2: 左侧视角 (left)           │
│  Camera 3: 右侧视角 (right)          │
│  Camera 4: 前方视角 (front)          │
│  Camera 5: 腕部相机 (wrist)          │
│  Camera 6: 第三人称 (third-person)   │
└─────────────────────────────────────┘

优势:
✅ 3D空间理解更全面
✅ 遮挡问题能被其他视角弥补
✅ 深度信息的隐式学习
```

#### (2) 为什么用EfficientNet而非ViT？
- **计算效率**: EfficientNet参数少，推理快
- **实时性**: 机器人控制要求<100ms延迟
- **性能**: 在机器人数据上表现良好

#### (3) Action Tokenization的优势
```
连续动作 vs 离散动作:

连续 (MSE Loss):
  pros: 精确控制
  cons: 训练不稳定，易过拟合

离散 (CrossEntropy):
  pros: 训练稳定，泛化好
  cons: 精度有限 (256 bins)

RT-1选择: 离散化
  → 实验证明泛化能力更强
```

---

### 2.3 训练细节

**数据集: RT-1 Robot Data**
```
规模:
  - 130,000 episodes
  - 13个机器人同时收集
  - 700种技能/任务
  - 17个月的时间

数据格式:
episode = {
    'images': [T, 6, 300, 300, 3],      # 时序图像序列
    'language': "pick up the apple",     # 语言指令
    'actions': [T, 7],                   # 动作序列
    'rewards': [T],                      # 成功/失败标签
}
```

**损失函数:**
```python
# 每个动作维度独立的交叉熵损失
loss = 0
for i in range(7):  # 7个动作维度
    loss += F.cross_entropy(
        pred_actions[:, i],  # [B, 256]
        target_actions[:, i]  # [B]
    )
loss = loss / 7
```

**训练策略:**

- Behavior Cloning (模仿学习)
- Batch size: 256
- Learning rate: 1e-4 with cosine decay
- 训练时长: ~1周 (TPU v3)

---

## 3. RT-2 架构详解

### 3.1 核心创新

**RT-2 = RT-1 + Vision-Language Model (VLM)**

关键思想: 利用大规模网络图文对预训练的视觉-语言知识

```
┌──────────────────────────────────────────────────┐
│       RT-2: 从网络知识到机器人控制                   │
└──────────────────────────────────────────────────┘

阶段1: VLM预训练 (网络数据)
┌─────────────┐
│  Image-Text │ → 预训练模型 (例如: PaLI, PaLM-E)
│  Pairs      │    学到: 视觉-语言对齐
│  数百万条   │           物体识别、场景理解
└─────────────┘

阶段2: 机器人微调 (RT-1数据)
┌─────────────┐
│ Robot Data  │ → 微调VLM → RT-2
│ 130K eps    │    适配: 语言→动作映射
└─────────────┘

结果: 强大的zero-shot泛化能力
```

---

### 3.2 RT-2 详细架构

```
┌────────────────────────────────────────────────────────────────┐
│                     RT-2 Architecture                           │
└────────────────────────────────────────────────────────────────┘

输入:
┌─────────────┐  ┌─────────────┐
│   Image     │  │  Language   │
│ (RGB)       │  │ Instruction │
│ 224x224x3   │  │             │
└──────┬──────┘  └──────┬──────┘
       │                │
       ▼                ▼
┌──────────────────────────────────────────────────┐
│     Vision-Language Model (PaLI/PaLM-E)          │
│                                                  │
│  ┌────────────────────────────────────┐         │
│  │    Vision Transformer (ViT)        │         │
│  │    - Pretrained on ImageNet        │         │
│  │    - Output: [196, 1024]           │         │
│  └──────────────┬─────────────────────┘         │
│                 │                                │
│  ┌──────────────┴─────────────────────┐         │
│  │   Language Model (Encoder-Decoder) │         │
│  │    - T5/PaLM architecture          │         │
│  │    - Cross-attention to vision     │         │
│  │                                    │         │
│  │   Input: "pick the red cup"        │         │
│  │   Visual Context: image features   │         │
│  │                                    │         │
│  │   ┌──────────────────────┐         │         │
│  │   │  Encoder (12 layers) │         │         │
│  │   └──────────┬───────────┘         │         │
│  │              ▼                      │         │
│  │   ┌──────────────────────┐         │         │
│  │   │  Decoder (12 layers) │         │         │
│  │   │  + Cross-Attention   │         │         │
│  │   └──────────────────────┘         │         │
│  └────────────────────────────────────┘         │
│                                                  │
│  Output: Contextualized Representation           │
│          [seq_len, 1024]                         │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│          Action Token Prediction                 │
│                                                  │
│  方法1: 继续用RT-1的tokenization                  │
│    → 7个分类头，每个256类                         │
│                                                  │
│  方法2: 文本生成动作 (Text-to-Action)             │
│    → 将动作表示为文本                             │
│    → 例: "1 165 224 128 50 200 0"                │
│    → 用VLM的decoder直接生成                       │
│                                                  │
│  RT-2采用: 方法1 (更稳定)                         │
└──────────────────┬───────────────────────────────┘
                   ▼

输出:
┌──────────────────────────────────────────────────┐
│    Action Vector: [x, y, z, r, p, y, gripper]    │
└──────────────────────────────────────────────────┘
```

---

### 3.3 RT-2的关键改进

#### (1) 预训练带来的zero-shot能力

```
示例任务对比:

┌─────────────────────────────────────────────┐
│  Task: "Pick up the extinct animal"         │
├─────────────────────────────────────────────┤
│  RT-1 (无预训练):                            │
│    → 失败 (未见过"extinct animal"这个词)       │
│                                             │
│  RT-2 (VLM预训练):                           │
│    → 成功识别玩具恐龙                          │
│    → 网络知识: 恐龙是灭绝动物                   │
│    → 视觉识别: 这是恐龙玩具                     │
│    → 动作生成: 抓取它                          │
└─────────────────────────────────────────────┘

新场景泛化:
  RT-1: 55% 成功率
  RT-2: 72% 成功率 (+17%)
```

#### (2) 语言理解的提升

```
复杂指令理解:

简单指令:
  "Pick the apple"  ← RT-1和RT-2都ok

推理指令:
  "Pick the healthiest snack"
  → RT-2能识别水果>薯片
  
上下文指令:
  "I'm thirsty, help me"
  → RT-2能推断出拿水杯
  
否定指令:
  "Pick anything except the red one"
  → RT-2能理解"不要红色的"
```

#### (3) 多任务统一表示

```
RT-2的统一范式:

任务类型1: 机器人控制
  Input: Image + "pick cup"
  Output: Action tokens

任务类型2: VQA (视觉问答)
  Input: Image + "what color is it?"
  Output: Text tokens

任务类型3: Captioning
  Input: Image
  Output: Text description

→ 同一个模型处理所有任务
→ 通过prompt区分任务类型
```

---

### 3.4 RT-2 训练流程

```
┌────────────────────────────────────────────────┐
│         RT-2 Two-Stage Training                │
└────────────────────────────────────────────────┘

Stage 1: VLM Pre-training (冻结大部分参数)
───────────────────────────────────────────────
Data: 网络图文对 (WebLI, CC, etc.)
      数百万条样本

Objective: 
  - Image-Text Matching
  - Image Captioning
  - Visual Question Answering

Duration: 数周 (大规模GPU集群)

Output: 预训练VLM权重
        ↓

Stage 2: Robot Fine-tuning
───────────────────────────────────────────────
Data: RT-1 Robot Data (130K episodes)

Method: 
  Option A: 微调所有参数
  Option B: 冻结encoder,只微调decoder
  Option C: LoRA (推荐)

Objective:
  - 动作预测 (主任务)
  - 保持VQA能力 (辅助任务)
  
Multi-task Loss:
  L = L_action + 0.1 * L_vqa

Duration: 数天

Output: RT-2 Final Model
```

**代码示例:**
```python
# RT-2 Fine-tuning (伪代码)
class RT2Model(nn.Module):
    def __init__(self, pretrained_vlm):
        self.vlm = pretrained_vlm  # PaLI/PaLM-E
        self.action_head = ActionHead(hidden_dim=1024, num_actions=7*256)
        
        # 冻结VLM的大部分参数
        for name, param in self.vlm.named_parameters():
            if 'decoder' not in name:
                param.requires_grad = False
    
    def forward(self, image, text, mode='action'):
        # VLM编码
        visual_features = self.vlm.vision_encoder(image)
        text_features = self.vlm.text_encoder(text)
        
        # 融合
        fused = self.vlm.decoder(text_features, visual_features)
        
        if mode == 'action':
            # 机器人动作预测
            actions = self.action_head(fused)
            return actions
        elif mode == 'vqa':
            # 保持VQA能力
            answer = self.vlm.lm_head(fused)
            return answer

# 训练循环
for batch in dataloader:
    if batch['type'] == 'robot':
        loss = action_loss(model(batch['image'], batch['text'], 'action'), 
                          batch['action'])
    else:  # VQA数据 (混合训练)
        loss = vqa_loss(model(batch['image'], batch['question'], 'vqa'),
                       batch['answer'])
```

---

## 4. RT-1 vs RT-2 对比

| 维度 | RT-1 | RT-2 |
|------|------|------|
| **基础架构** | EfficientNet + Transformer | VLM (ViT + T5/PaLM) |
| **预训练数据** | 无 (纯机器人数据) | 数百万网络图文对 |
| **参数量** | ~35M | ~5B (VLM backbone) |
| **训练集** | 130K episodes | 130K episodes + web data |
| **Zero-shot能力** | 弱 (55%) | 强 (72%) |
| **语言理解** | 基础 | 高级 (推理、上下文) |
| **推理速度** | 快 (~3Hz) | 慢 (~1Hz) |
| **部署难度** | 低 | 高 (模型大) |
| **适用场景** | 见过的任务 | 新颖任务 |

---

## 5. 实现要点

### 5.1 数据预处理

```python
import torchvision.transforms as T

# RT-1 数据增强
transform = T.Compose([
    T.RandomCrop(300, padding=30),           # 随机裁剪
    T.ColorJitter(brightness=0.3,            # 颜色抖动
                  contrast=0.3, 
                  saturation=0.3),
    T.RandomHorizontalFlip(p=0.5),          # 水平翻转
    T.Normalize(mean=[0.485, 0.456, 0.406],  # 归一化
                std=[0.229, 0.224, 0.225])
])

# RT-2 使用VLM的预处理
from transformers import AutoImageProcessor
processor = AutoImageProcessor.from_pretrained("google/paligemma-3b-pt-224")
```

### 5.2 Action De-tokenization

```python
def detokenize_action(action_tokens, action_ranges):
    """
    将离散token转回连续值
    
    Args:
        action_tokens: [7] 整数数组，每个值0-255
        action_ranges: dict, 每个维度的范围
    
    Returns:
        continuous_action: [7] 连续值
    """
    action = []
    for i, (name, (min_val, max_val)) in enumerate(action_ranges.items()):
        token = action_tokens[i]
        # 线性映射: token ∈ [0, 255] → value ∈ [min, max]
        value = min_val + (token / 255.0) * (max_val - min_val)
        action.append(value)
    return np.array(action)

# 示例
ranges = {
    'x': (-0.5, 0.5),
    'y': (-0.5, 0.5),
    'z': (0.0, 0.8),
    'roll': (-3.14, 3.14),
    'pitch': (-3.14, 3.14),
    'yaw': (-3.14, 3.14),
    'gripper': (0, 1)
}

tokens = [128, 200, 100, 150, 128, 200, 255]  # 预测的tokens
action = detokenize_action(tokens, ranges)
print(action)  # [0.002, 0.176, 0.314, 0.872, 0.0, 1.725, 1.0]
```

### 5.3 实时推理优化

```python
import torch
import time

class RT1Inference:
    def __init__(self, model_path):
        self.model = torch.jit.load(model_path)  # TorchScript加速
        self.model.eval()
        self.model.cuda()
        
        # 预分配缓冲区
        self.image_buffer = torch.zeros(1, 6, 300, 300, 3).cuda()
        
    @torch.no_grad()
    def predict(self, images, text):
        start = time.time()
        
        # 图像预处理
        images = self.preprocess(images)  # 5ms
        
        # 模型推理
        action_tokens = self.model(images, text)  # 30ms
        
        # De-tokenize
        action = self.detokenize(action_tokens)  # 1ms
        
        latency = (time.time() - start) * 1000
        print(f"Inference latency: {latency:.1f}ms")
        
        return action
```

---

## 6. 实际部署经验

### 6.1 硬件需求

```
RT-1 部署:
┌─────────────────────────────────────┐
│ CPU: Intel i7 (8核)                 │
│ GPU: NVIDIA RTX 3080 (10GB)         │
│ RAM: 32GB                           │
│ 相机: 6×RealSense D435 (RGB-D)      │
│ 机器人: 7-DoF机械臂                  │
└─────────────────────────────────────┘
推理频率: ~3Hz
功耗: ~250W

RT-2 部署:
┌─────────────────────────────────────┐
│ CPU: AMD EPYC (32核)                │
│ GPU: NVIDIA A100 (40GB)             │
│ RAM: 128GB                          │
│ 相机: 同上                           │
└─────────────────────────────────────┘
推理频率: ~1Hz
功耗: ~400W
```

### 6.2 常见问题与解决

**问题1: 推理太慢**
```
原因: 模型太大/未优化
解决:
  ✅ 使用TensorRT量化(INT8)
  ✅ 模型剪枝 (去掉冗余层)
  ✅ 知识蒸馏 (大模型→小模型)
  ✅ 批处理多个请求
```

**问题2: 动作抖动**
```
原因: 逐帧预测导致不连续
解决:
  ✅ 时序滤波 (指数移动平均)
  ✅ 使用action horizon (预测多步)
  ✅ PID控制器平滑
```

**问题3: 新物体泛化差**
```
原因: 训练数据不足
解决:
  ✅ 使用RT-2的预训练知识
  ✅ 在线适应 (few-shot微调)
  ✅ 数据增强 (合成新物体)
```

---

## 7. 面试重点问题

**Q1: RT-1为什么用action tokenization而不是直接回归？**  
A: 离散化使训练更稳定，交叉熵损失收敛更快，泛化能力更强。256个bins足够机器人控制精度。

**Q2: RT-2如何保持VLM能力不被遗忘？**  
A: 多任务学习，同时训练机器人动作和VQA任务。损失函数加权: L = L_action + λ·L_vqa

**Q3: TokenLearner的作用是什么？**  
A: 将大量视觉tokens(488个)压缩到少量learned tokens(8个)，减少Transformer计算量，同时保留关键信息。

**Q4: RT系列模型如何处理失败case？**  
A: 

- 训练时混合成功/失败样本
- 推理时加置信度阈值
- 失败后重新规划或人工介入

**Q5: 如何评估RT模型的性能？**  
A: 

- Success Rate (成功率)
- Generalization Gap ( 泛化距离=训练场景成功率-新场景成功率）
- Inference Latency (推理延迟)
- Sample Efficiency (数据效率)

---

## 8. 参考资料

### 论文
1. **RT-1**: https://arxiv.org/abs/2212.06817
2. **RT-2**: https://arxiv.org/abs/2307.15818

### 代码
- TensorFlow官方实现: https://github.com/google-research/robotics_transformer
- PyTorch复现: https://github.com/kyegomez/RT-2

### 博客
- Google AI Blog: RT-1解析
- DeepMind Blog: 具身AI的未来

---

*文档版本: v1.0*  
*最后更新: 2025-11-07*

