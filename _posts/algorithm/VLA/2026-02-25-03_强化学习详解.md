---
layout: post
title: 03_强化学习详解
categories: [algorithm,VLA]
description: 
keywords: 
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---


# 强化学习算法详解 - 机器人应用

## 1. 强化学习基础框架

### 1.1 核心概念

```
┌────────────────────────────────────────────────────┐
│          强化学习交互循环 (RL Loop)                   │
└────────────────────────────────────────────────────┘

        ┌──────────────────────┐
        │    Environment       │
        │   (机器人+世界)       │
        └──────────┬───────────┘
                   │
                   │ State (s_t)
                   │ 状态: [位置, 速度, 图像, ...]
                   ▼
        ┌──────────────────────┐
        │      Agent           │
        │    (策略网络)         │
        │   π(a|s) or Q(s,a)   │
        └──────────┬───────────┘
                   │
                   │ Action (a_t)
                   │ 动作: [关节角度, 力矩, ...]
                   ▼
        ┌──────────────────────┐
        │   Environment        │
        │   执行动作             │
        └──────────┬───────────┘
                   │
                   │ Reward (r_t)
                   │ 奖励: 成功=+1, 失败=-1
                   │
                   │ Next State (s_{t+1})
                   │ 新状态
                   ▼
        ┌──────────────────────┐
        │   Experience Buffer  │
        │   (s, a, r, s')      │
        └──────────┬───────────┘
                   │
                   │ Sample Batch
                   ▼
        ┌──────────────────────┐
        │   Update Policy      │
        │   (梯度下降)           │
        └──────────────────────┘
```

### 1.2 RL要素

| 要素 | 符号 | 机器人示例 | 维度 |
|------|------|-----------|------|
| **状态** | s | 关节角度+图像+物体位置 | 高维 |
| **动作** | a | 关节速度/力矩 | 连续/离散 |
| **奖励** | r | 任务完成度 | 标量 |
| **策略** | π(a\|s) | 神经网络映射s→a | 概率分布 |
| **价值** | V(s), Q(s,a) | 状态/动作的期望回报 | 标量 |

---

## 2. 核心算法架构

### 2.1 DQN (Deep Q-Network)

```
┌────────────────────────────────────────────────────┐
│              DQN Architecture                       │
└────────────────────────────────────────────────────┘

输入: State (s)
┌──────────────┐
│  State s_t   │  例: [joint_angles(7), gripper(1), 
│              │       image_features(512)]
└──────┬───────┘
       │
       ▼
┌──────────────────────────────────────────────┐
│        Q-Network (主网络)                     │
│                                              │
│  ┌─────────────────────────────┐             │
│  │   Input Layer (520 dims)    │             │
│  └────────────┬────────────────┘             │
│               ▼                              │
│  ┌─────────────────────────────┐            │
│  │   Hidden Layer 1 (256)      │            │
│  │   + ReLU                    │            │
│  └────────────┬────────────────┘            │
│               ▼                              │
│  ┌─────────────────────────────┐            │
│  │   Hidden Layer 2 (256)      │            │
│  │   + ReLU                    │            │
│  └────────────┬────────────────┘            │
│               ▼                              │
│  ┌─────────────────────────────┐            │
│  │   Output Layer (n_actions)  │            │
│  │   Q(s, a_1), ..., Q(s, a_n) │            │
│  └─────────────────────────────┘            │
│                                              │
│  例: 对于离散动作空间                         │
│    - 向左移动: Q(s, left) = 0.8             │
│    - 向右移动: Q(s, right) = 0.3            │
│    - 抓取:    Q(s, grasp) = 1.5  ← 最大     │
└──────────────────────────────────────────────┘
       │
       │ 选择动作: a = argmax_a Q(s, a)
       ▼
执行动作 → 获得 (s', r)

┌──────────────────────────────────────────────┐
│        Target Q-Network (目标网络)            │
│                                              │
│  结构与主网络完全相同                         │
│  但参数每C步同步一次: θ' ← θ                 │
│  目的: 稳定训练                              │
└──────────────────────────────────────────────┘

训练:
┌──────────────────────────────────────────────┐
│   Loss Function (TD Error)                   │
│                                              │
│   L = E[(y - Q(s,a;θ))²]                     │
│                                              │
│   其中 y = r + γ·max_a' Q(s', a'; θ')       │
│         ↑       ↑                            │
│      即时奖励  未来期望(用target网络计算)     │
│                                              │
│   梯度更新: θ ← θ - α·∇_θ L                  │
└──────────────────────────────────────────────┘

关键技巧:
┌──────────────────────────────────────────────┐
│ 1. Experience Replay (经验回放)              │
│    Buffer: [(s,a,r,s'), ...] 容量: 1M       │
│    每步随机采样batch进行训练                 │
│    → 打破数据相关性                          │
│                                              │
│ 2. Target Network (目标网络)                 │
│    每1000步同步: θ_target ← θ               │
│    → 避免Q值震荡                             │
│                                              │
│ 3. ε-greedy Exploration (探索策略)           │
│    以概率ε随机动作，1-ε选最优动作            │
│    ε从1.0 → 0.01 (退火)                     │
└──────────────────────────────────────────────┘
```

**代码实现:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, state):
        return self.network(state)  # [B, action_dim]

class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.q_network = DQN(state_dim, action_dim).cuda()
        self.target_network = DQN(state_dim, action_dim).cuda()
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-4)
        self.replay_buffer = deque(maxlen=100000)
        
        self.epsilon = 1.0  # 探索率
        self.gamma = 0.99   # 折扣因子
        
    def select_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, action_dim-1)  # 随机探索
        else:
            with torch.no_grad():
                q_values = self.q_network(state)
                return q_values.argmax().item()  # 贪婪选择
    
    def train(self, batch_size=32):
        if len(self.replay_buffer) < batch_size:
            return
        
        # 采样batch
        batch = random.sample(self.replay_buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.stack(states).cuda()
        actions = torch.tensor(actions).cuda()
        rewards = torch.tensor(rewards).cuda()
        next_states = torch.stack(next_states).cuda()
        dones = torch.tensor(dones).cuda()
        
        # 计算当前Q值
        q_values = self.q_network(states)
        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze()
        
        # 计算目标Q值
        with torch.no_grad():
            next_q_values = self.target_network(next_states)
            next_q_value = next_q_values.max(1)[0]
            target_q_value = rewards + self.gamma * next_q_value * (1 - dones)
        
        # 更新Q网络
        loss = nn.MSELoss()(q_value, target_q_value)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # 衰减探索率
        self.epsilon = max(0.01, self.epsilon * 0.995)
```

---

### 2.2 PPO (Proximal Policy Optimization)

```
┌────────────────────────────────────────────────────┐
│           PPO Architecture (Actor-Critic)           │
└────────────────────────────────────────────────────┘

输入: State (s)
┌──────────────┐
│  State s_t   │
└──────┬───────┘
       │
       ├───────────────────────────┐
       │                           │
       ▼                           ▼
┌──────────────────┐     ┌──────────────────┐
│  Actor Network   │     │  Critic Network  │
│  (策略网络)       │     │  (价值网络)       │
└──────────────────┘     └──────────────────┘

Actor Network:
┌──────────────────────────────────────────────┐
│  输入: State [B, state_dim]                  │
│         ↓                                    │
│  ┌─────────────────┐                        │
│  │  FC1 (256)      │                        │
│  │  + Tanh         │                        │
│  └────────┬────────┘                        │
│           ▼                                  │
│  ┌─────────────────┐                        │
│  │  FC2 (256)      │                        │
│  │  + Tanh         │                        │
│  └────────┬────────┘                        │
│           ▼                                  │
│  ┌─────────────────────────────┐            │
│  │  Mean Head (action_dim)     │            │
│  │  μ(s) = 均值                │            │
│  └─────────────────────────────┘            │
│           │                                  │
│  ┌─────────────────────────────┐            │
│  │  Log_std Head (action_dim)  │            │
│  │  log σ(s) = 对数标准差      │            │
│  └─────────────────────────────┘            │
│           │                                  │
│  输出: 高斯分布 N(μ(s), σ(s))               │
│        采样动作: a ~ N(μ, σ)                │
└──────────────────────────────────────────────┘

Critic Network:
┌──────────────────────────────────────────────┐
│  输入: State [B, state_dim]                  │
│         ↓                                    │
│  ┌─────────────────┐                        │
│  │  FC1 (256)      │                        │
│  │  + Tanh         │                        │
│  └────────┬────────┘                        │
│           ▼                                  │
│  ┌─────────────────┐                        │
│  │  FC2 (256)      │                        │
│  │  + Tanh         │                        │
│  └────────┬────────┘                        │
│           ▼                                  │
│  ┌─────────────────┐                        │
│  │  Value Head (1) │                        │
│  │  V(s) = 状态价值│                        │
│  └─────────────────┘                        │
│           │                                  │
│  输出: 标量 V(s)                            │
└──────────────────────────────────────────────┘

训练流程:
┌──────────────────────────────────────────────┐
│ 1. 收集数据 (Rollout)                        │
│    for t in range(T):                        │
│        a_t ~ π_old(·|s_t)  # 旧策略采样     │
│        s_{t+1}, r_t = env.step(a_t)         │
│        存储 (s_t, a_t, r_t)                 │
│                                              │
│ 2. 计算优势函数 (Advantage)                  │
│    A_t = r_t + γV(s_{t+1}) - V(s_t)         │
│    或使用GAE: A_t = Σ(γλ)^k δ_{t+k}        │
│                                              │
│ 3. PPO更新 (多轮epoch)                       │
│    for epoch in range(K):                    │
│        # Actor loss (clipped)               │
│        ratio = π_new(a|s) / π_old(a|s)      │
│        L_clip = min(                         │
│            ratio * A,                        │
│            clip(ratio, 1-ε, 1+ε) * A        │
│        )                                     │
│        L_actor = -E[L_clip]                 │
│                                              │
│        # Critic loss                         │
│        L_critic = E[(V(s) - V_target)²]     │
│                                              │
│        # 总损失                              │
│        L = L_actor + c1*L_critic - c2*H     │
│           ↑         ↑          ↑            │
│         策略损失  价值损失   熵正则(探索)    │
└──────────────────────────────────────────────┘

关键: PPO的Clipping
┌──────────────────────────────────────────────┐
│   防止策略更新过大                            │
│                                              │
│   ratio = π_new(a|s) / π_old(a|s)           │
│                                              │
│   if A > 0 (好动作):                         │
│     希望增大概率,但不超过 (1+ε)              │
│   if A < 0 (坏动作):                         │
│     希望减小概率,但不低于 (1-ε)              │
│                                              │
│   ε 通常取 0.1 或 0.2                        │
└──────────────────────────────────────────────┘
```

**代码实现:**
```python
class PPOAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = ActorNetwork(state_dim, action_dim).cuda()
        self.critic = CriticNetwork(state_dim).cuda()
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)
        
        self.clip_epsilon = 0.2
        self.gamma = 0.99
        self.lam = 0.95  # GAE λ
        
    def select_action(self, state):
        with torch.no_grad():
            mean, log_std = self.actor(state)
            std = log_std.exp()
            dist = torch.distributions.Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(-1)
        return action, log_prob
    
    def compute_gae(self, rewards, values, next_value, dones):
        """计算广义优势估计 (GAE)"""
        advantages = []
        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_val * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages).cuda()
    
    def update(self, states, actions, old_log_probs, returns, advantages):
        """PPO更新"""
        for _ in range(10):  # K个epoch
            # Actor loss
            mean, log_std = self.actor(states)
            std = log_std.exp()
            dist = torch.distributions.Normal(mean, std)
            new_log_probs = dist.log_prob(actions).sum(-1)
            entropy = dist.entropy().sum(-1).mean()
            
            ratio = (new_log_probs - old_log_probs).exp()
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy
            
            # Critic loss
            values = self.critic(states)
            critic_loss = (returns - values).pow(2).mean()
            
            # 反向传播
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.actor_optimizer.step()
            
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
```

---

### 2.3 SAC (Soft Actor-Critic)

```
┌────────────────────────────────────────────────────┐
│         SAC Architecture (最大熵RL)                 │
└────────────────────────────────────────────────────┘

核心思想: 最大化期望奖励 + 策略熵
  → 鼓励探索,提高鲁棒性

目标函数:
  J(π) = E[Σ r_t + α·H(π(·|s_t))]
                    ↑
                 熵奖励(探索)

网络结构:
┌──────────────┐
│  State (s)   │
└──────┬───────┘
       │
       ├────────────────────────────────┐
       │                                │
       ▼                                ▼
┌──────────────────┐         ┌──────────────────┐
│  Actor (Policy)  │         │  Critic (Q-nets) │
│                  │         │                  │
│  π(a|s) →       │         │  Q1(s,a)         │
│  mean, log_std   │         │  Q2(s,a)         │
│                  │         │  (双Q网络)        │
└──────────────────┘         └──────────────────┘
       │                                │
       │ 采样 a ~ N(μ,σ)                │
       └────────────┬───────────────────┘
                    ▼
              计算Q值和策略损失

Actor Network (同PPO):
  Input: s → FC → FC → (mean, log_std)
  Output: a ~ N(mean, std)
  
  特殊: Tanh Squashing
    → a_raw ~ N(μ, σ)
    → a_final = tanh(a_raw)  # 限制在[-1, 1]

Critic Network (Double Q):
  Input: (s, a) → concat → FC → FC → Q(s,a)
  
  为什么双Q?
    → 减少过估计偏差
    → Q_target = min(Q1, Q2)

训练:
┌──────────────────────────────────────────────┐
│ 1. Actor Loss (策略梯度)                     │
│    L_π = E[α·log π(a|s) - Q(s,a)]          │
│           ↑              ↑                   │
│        熵惩罚         Q值指导                 │
│                                              │
│ 2. Critic Loss (TD error)                    │
│    y = r + γ(min(Q1',Q2') - α·log π(a'|s')) │
│    L_Q = (Q(s,a) - y)²                      │
│                                              │
│ 3. Temperature (α) 自动调整                  │
│    L_α = -α(log π(a|s) + H_target)          │
│    → 自适应控制探索程度                      │
└──────────────────────────────────────────────┘
```

---

## 3. 机器人RL的特殊挑战

### 3.1 奖励函数设计

```
┌────────────────────────────────────────────────────┐
│         机器人抓取任务的奖励设计                    │
└────────────────────────────────────────────────────┘

任务: 抓取桌上的杯子

❌ 简单稀疏奖励 (不推荐):
  r = +1  if success
  r = 0   otherwise
  
  问题: 难以学习 (成功概率<1%)

✅ 稠密shaped奖励 (推荐):
  r_total = r_reach + r_grasp + r_lift + r_success
  
  r_reach = -||gripper_pos - object_pos||  # 接近物体
  r_grasp = {
    +0.1  if gripper_close and touching
    0     otherwise
  }
  r_lift = max(0, object_height - initial_height)  # 抬高物体
  r_success = {
    +10   if object above threshold
    0     otherwise
  }
  
  额外惩罚:
    -0.01 * ||action||²  # 动作平滑性
    -1 if collision       # 碰撞惩罚

示例代码:
def compute_reward(state, action):
    gripper_pos = state['gripper_position']
    object_pos = state['object_position']
    object_height = state['object_height']
    initial_height = 0.0
    
    # 距离奖励
    dist = np.linalg.norm(gripper_pos - object_pos)
    r_reach = -dist
    
    # 抓取奖励
    r_grasp = 0.1 if (state['gripper_force'] > 0.5 and dist < 0.05) else 0
    
    # 抬起奖励
    r_lift = max(0, object_height - initial_height) * 2.0
    
    # 成功奖励
    r_success = 10.0 if object_height > 0.3 else 0
    
    # 动作惩罚
    r_action = -0.01 * np.sum(action ** 2)
    
    return r_reach + r_grasp + r_lift + r_success + r_action
```

### 3.2 状态空间设计

```
机器人状态表示:

方案1: 低维状态 (适合简单任务)
┌────────────────────────────────────┐
│ s = [joint_angles (7),             │
│      joint_velocities (7),         │
│      gripper_state (1),            │
│      object_position (3),          │
│      object_orientation (4)]       │
│                                    │
│ 总维度: 22                         │
└────────────────────────────────────┘
优点: 训练快,样本效率高
缺点: 需要精确感知 (mocap)

方案2: 高维视觉 (复杂场景)
┌────────────────────────────────────┐
│ s = [RGB image (84x84x3),          │
│      depth image (84x84x1),        │
│      proprioception (8)]           │
│                                    │
│ 总维度: 84x84x4 + 8 = 28,232       │
└────────────────────────────────────┘
优点: 泛化能力强,不依赖精确感知
缺点: 样本需求大 (百万级)

方案3: 混合表示 (推荐)
┌────────────────────────────────────┐
│ s = [CNN_features(image) (512),    │
│      proprioception (8)]           │
│                                    │
│ 总维度: 520                        │
└────────────────────────────────────┘
优点: 平衡性能和效率
实现: 预训练CNN提取视觉特征
```

---

## 4. 模仿学习 (Imitation Learning)

### 4.1 Behavior Cloning (BC)

```
┌────────────────────────────────────────────────────┐
│           Behavior Cloning 流程                     │
└────────────────────────────────────────────────────┘

步骤1: 收集专家演示
┌──────────────────────────────┐
│  Human Teleoperation         │
│  或 Motion Planning Algorithm│
│                              │
│  收集: D = {(s_i, a_i)}      │
│  数量: 通常需要数百-数千条     │
└──────────────────────────────┘
       │
       ▼
步骤2: 监督学习
┌──────────────────────────────┐
│  Train Policy π_θ(a|s)       │
│                              │
│  Loss: L = Σ||π_θ(s_i)-a_i||²│
│            ↑                 │
│     MSE loss (连续动作)       │
│  或 CrossEntropy (离散动作)   │
└──────────────────────────────┘
       │
       ▼
步骤3: 部署测试
┌──────────────────────────────┐
│  a_t = π_θ(s_t)              │
│  执行并评估                   │
└──────────────────────────────┘

优点:
  ✅ 简单,易实现
  ✅ 不需要奖励函数
  ✅ 收敛快

缺点:
  ❌ 分布偏移 (Distributional Shift)
  ❌ 复合误差 (Compounding Errors)
  ❌ 数据需求大
```

**代码实现:**
```python
class BehaviorCloningAgent:
    def __init__(self, state_dim, action_dim):
        self.policy = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()  # 假设动作在[-1,1]
        ).cuda()
        
        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)
    
    def train(self, expert_data, epochs=100):
        """
        expert_data: List of (state, action) tuples
        """
        states = torch.stack([d[0] for d in expert_data]).cuda()
        actions = torch.stack([d[1] for d in expert_data]).cuda()
        
        dataset = torch.utils.data.TensorDataset(states, actions)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)
        
        for epoch in range(epochs):
            total_loss = 0
            for batch_states, batch_actions in dataloader:
                pred_actions = self.policy(batch_states)
                loss = F.mse_loss(pred_actions, batch_actions)
                
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}")
```

### 4.2 DAgger (Dataset Aggregation)

```
解决BC的分布偏移问题

┌────────────────────────────────────────────────────┐
│               DAgger 迭代流程                       │
└────────────────────────────────────────────────────┘

初始化: D = 专家演示数据

for iteration in 1 to N:
    1. 用当前策略π_i收集数据
       ┌──────────────────────┐
       │  s_t = π_i执行得到   │
       └──────────────────────┘
    
    2. 专家标注动作
       ┌──────────────────────┐
       │  让专家看到s_t        │
       │  专家给出: a_t^expert│
       └──────────────────────┘
    
    3. 聚合数据集
       ┌──────────────────────┐
       │  D = D ∪ {(s_t, a_t)}│
       └──────────────────────┘
    
    4. 重新训练策略
       ┌──────────────────────┐
       │  π_{i+1} ← train(D)  │
       └──────────────────────┘

优点: 逐渐覆盖策略访问的状态空间
缺点: 需要专家持续标注 (昂贵)
```

---

## 5. 离线强化学习 (Offline RL)

```
┌────────────────────────────────────────────────────┐
│      Offline RL vs Online RL                        │
└────────────────────────────────────────────────────┘

Online RL:
  Agent ⇄ Environment (实时交互)
  优点: 可以探索,数据分布可控
  缺点: 安全性问题,成本高

Offline RL:
  Agent → Fixed Dataset (不与环境交互)
  优点: 安全,可利用历史数据
  缺点: 分布外泛化难

┌────────────────────────────────────────────────────┐
│       Conservative Q-Learning (CQL)                 │
└────────────────────────────────────────────────────┘

核心思想: 惩罚分布外动作的Q值

标准Q-learning:
  L = (Q(s,a) - y)²
  
CQL:
  L = (Q(s,a) - y)² + α·[E_{a~π}[Q(s,a)] - E_{a~D}[Q(s,a)]]
                           ↑                    ↑
                      当前策略下的Q值        数据集中的Q值
                      (惩罚,降低)            (提升)

效果: Q网络保守估计,避免过高估计未见过的动作
```

---

## 6. 面试高频问题

**Q1: DQN的两个关键技巧是什么?**  
A: (1) Experience Replay: 打破数据相关性; (2) Target Network: 稳定Q值更新

**Q2: PPO相比TRPO有什么优势?**  
A: PPO用clipping代替TRPO的复杂KL约束,更简单易实现,且性能相当

**Q3: 为什么机器人RL需要稠密奖励?**  
A: 稀疏奖励导致探索困难,稠密奖励提供梯度信息,加速学习

**Q4: 如何解决RL的样本效率问题?**  
A: (1) 模仿学习预训练; (2) Sim2Real; (3) 模型融合; (4) 离线RL

**Q5: Actor-Critic的优势是什么?**  
A: Actor负责策略,Critic负责价值估计,减少方差,加速收敛

---

## 7. 参考资源

- 经典教材: Sutton & Barto《Reinforcement Learning: An Introduction》
- 课程: UC Berkeley CS285 (Deep RL)
- 库: Stable-Baselines3, RLlib

---

*文档版本: v1.0*  
*最后更新: 2025-11-07*

