---
layout: post
title: 04_多模态融合详解
categories: [algorithm]
description: 
keywords: 
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---


# 多模态融合技术详解

## 1. 多模态感知概述

### 1.1 机器人的感知模态

```
┌────────────────────────────────────────────────────┐
│          机器人多模态感知系统                        │
└────────────────────────────────────────────────────┘

感知模态分类:
┌──────────────────────────────────────────────┐
│                                              │
│  外感受 (Exteroception)                      │
│  ├─ 视觉: RGB相机, 深度相机                   │
│  ├─ 触觉: 力/触觉传感器                       │
│  ├─ 听觉: 麦克风阵列                          │
│  └─ 其他: 激光雷达, IMU                       │
│                                              │
│  本体感受 (Proprioception)                   │
│  ├─ 关节位置: 编码器                          │
│  ├─ 关节速度: 速度传感器                      │
│  ├─ 关节力矩: 力矩传感器                      │
│  └─ 末端位姿: 正运动学计算                    │
│                                              │
└──────────────────────────────────────────────┘

数据特性对比:
┌────────┬──────────┬──────────┬──────────┐
│ 模态   │ 维度     │ 频率     │ 噪声     │
├────────┼──────────┼──────────┼──────────┤
│ RGB    │ 高(WxHx3)│ 30Hz     │ 光照敏感  │
│ 深度   │ 中(WxH)  │ 30Hz     │ 反射影响  │
│ 触觉   │ 低(6)    │ 100Hz    │ 漂移     │
│ 本体   │ 低(7-10) │ 1000Hz   │ 低噪声   │
│ 力反馈 │ 低(6)    │ 500Hz    │ 振动噪声  │
└────────┴──────────┴──────────┴──────────┘
```

---

## 2. 多模态融合架构

### 2.1 Early Fusion (早期融合)

```
┌────────────────────────────────────────────────────┐
│             Early Fusion Architecture               │
└────────────────────────────────────────────────────┘

特点: 在特征提取前融合原始数据

┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  RGB Image  │  │ Depth Image │  │Proprio State│
│ [3,224,224] │  │ [1,224,224] │  │    [10]     │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       │  Resize/Tile   │                │  Tile
       │  Depth到RGB维度│                │  [10]→[10,224,224]
       ▼                ▼                ▼
┌──────────────────────────────────────────────────┐
│          Concatenate Along Channel               │
│                                                  │
│   Combined = [RGB || Depth || Tiled_Proprio]    │
│   Shape: [3+1+10, 224, 224] = [14, 224, 224]    │
└──────────────────┬───────────────────────────────┘
                   ▼
┌──────────────────────────────────────────────────┐
│        Unified Feature Extractor (CNN)           │
│                                                  │
│   Conv1: [14, 224, 224] → [64, 112, 112]        │
│   Conv2: [64, 112, 112] → [128, 56, 56]         │
│   Conv3: [128, 56, 56] → [256, 28, 28]          │
│   ...                                            │
│   GlobalPool: [512, 7, 7] → [512]               │
└──────────────────┬───────────────────────────────┘
                   ▼
          Task-specific Head

优点:
  ✅ 简单易实现
  ✅ 端到端学习模态间低级特征交互

缺点:
  ❌ 不同模态的原始数据尺度差异大
  ❌ 难以处理异步数据
  ❌ 一个模态缺失时整个系统失效
```

**代码实现:**
```python
class EarlyFusionModel(nn.Module):
    def __init__(self, action_dim=7):
        super().__init__()
        
        # 统一CNN backbone
        self.backbone = nn.Sequential(
            nn.Conv2d(14, 64, 7, stride=2, padding=3),  # RGB(3)+Depth(1)+Proprio(10)
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.fc = nn.Linear(256, action_dim)
    
    def forward(self, rgb, depth, proprio):
        # RGB: [B, 3, 224, 224]
        # Depth: [B, 1, 224, 224]
        # Proprio: [B, 10]
        
        # Tile proprio to spatial dimensions
        proprio_tiled = proprio.unsqueeze(-1).unsqueeze(-1)  # [B, 10, 1, 1]
        proprio_tiled = proprio_tiled.expand(-1, -1, 224, 224)  # [B, 10, 224, 224]
        
        # Concatenate
        fused = torch.cat([rgb, depth, proprio_tiled], dim=1)  # [B, 14, 224, 224]
        
        # Extract features
        features = self.backbone(fused).squeeze(-1).squeeze(-1)  # [B, 256]
        
        # Predict action
        action = self.fc(features)
        return action
```

---

### 2.2 Late Fusion (后期融合)

```
┌────────────────────────────────────────────────────┐
│             Late Fusion Architecture                │
└────────────────────────────────────────────────────┘

特点: 每个模态独立提取特征,最后融合

┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  RGB Image  │  │ Depth Image │  │   Proprio   │
│ [3,224,224] │  │ [1,224,224] │  │    [10]     │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       ▼                ▼                ▼
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│  RGB Encoder │ │ Depth Encoder│ │ State Encoder│
│   (ResNet)   │ │   (ResNet)   │ │    (MLP)     │
│              │ │              │ │              │
│ [3,224,224]  │ │ [1,224,224]  │ │    [10]      │
│      ↓       │ │      ↓       │ │      ↓       │
│   Conv...    │ │   Conv...    │ │    FC...     │
│      ↓       │ │      ↓       │ │      ↓       │
│   [512]      │ │   [512]      │ │   [128]      │
└──────┬───────┘ └──────┬───────┘ └──────┬───────┘
       │                │                │
       │                │                │
       └────────────────┼────────────────┘
                        ▼
         ┌───────────────────────────────┐
         │   Fusion Module               │
         │                               │
         │   方案1: Concatenation        │
         │     [512 || 512 || 128]       │
         │     → [1152]                  │
         │                               │
         │   方案2: Weighted Sum          │
         │     w1·rgb + w2·depth + w3·st │
         │     (learnable weights)       │
         │                               │
         │   方案3: Gating               │
         │     gate = σ(W·[rgb,depth,st])│
         │     output = gate ⊙ features  │
         └───────────────┬───────────────┘
                         ▼
              ┌────────────────────┐
              │  Action Predictor  │
              │     FC Layers      │
              │         ↓          │
              │   Action [7]       │
              └────────────────────┘

优点:
  ✅ 模态独立,便于预训练
  ✅ 易于处理模态缺失
  ✅ 可解释性好

缺点:
  ❌ 可能丢失低级特征交互
  ❌ 参数量较大
```

**代码实现:**
```python
class LateFusionModel(nn.Module):
    def __init__(self, action_dim=7):
        super().__init__()
        
        # 独立编码器
        self.rgb_encoder = torchvision.models.resnet18(pretrained=True)
        self.rgb_encoder.fc = nn.Linear(512, 512)
        
        self.depth_encoder = torchvision.models.resnet18(pretrained=False)
        self.depth_encoder.conv1 = nn.Conv2d(1, 64, 7, 2, 3)  # 单通道输入
        self.depth_encoder.fc = nn.Linear(512, 512)
        
        self.proprio_encoder = nn.Sequential(
            nn.Linear(10, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU()
        )
        
        # 融合层
        self.fusion = nn.Sequential(
            nn.Linear(512 + 512 + 128, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU()
        )
        
        # 动作预测
        self.action_head = nn.Linear(256, action_dim)
    
    def forward(self, rgb, depth, proprio):
        # 分别编码
        rgb_feat = self.rgb_encoder(rgb)  # [B, 512]
        depth_feat = self.depth_encoder(depth)  # [B, 512]
        proprio_feat = self.proprio_encoder(proprio)  # [B, 128]
        
        # 拼接融合
        fused = torch.cat([rgb_feat, depth_feat, proprio_feat], dim=1)  # [B, 1152]
        
        # 进一步处理
        fused = self.fusion(fused)  # [B, 256]
        
        # 预测动作
        action = self.action_head(fused)
        return action
```

---

### 2.3 Attention-Based Fusion (注意力融合)

```
┌────────────────────────────────────────────────────┐
│        Cross-Attention Fusion (推荐)                │
└────────────────────────────────────────────────────┘

核心思想: 不同模态动态交互,学习重要性权重

架构:
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│RGB Features │  │Depth Features│ │Proprio Feat │
│  [B,N,D]    │  │  [B,M,D]     │ │  [B,1,D]    │
└──────┬──────┘  └──────┬───────┘ └──────┬──────┘
       │                │                │
       └────────────────┴────────────────┘
                        │
                        ▼
         ┌───────────────────────────────┐
         │   Multi-Head Cross-Attention  │
         │                               │
         │  Query: RGB Features          │
         │  Key:   All Features          │
         │  Value: All Features          │
         │                               │
         │  Attention(Q,K,V) =           │
         │    softmax(QK^T/√d) × V       │
         └───────────────┬───────────────┘
                         ▼
         ┌───────────────────────────────┐
         │   Enhanced RGB Features       │
         │   (融合了其他模态的信息)        │
         └───────────────┬───────────────┘
                         ▼
              ┌────────────────────┐
              │  Pooling + FC      │
              │      ↓             │
              │  Action [7]        │
              └────────────────────┘

详细计算:
┌──────────────────────────────────────────────┐
│  假设:                                        │
│    RGB tokens:   [B, 196, 768]  (ViT)       │
│    Depth tokens: [B, 196, 768]              │
│    Proprio:      [B, 1, 768]                │
│                                              │
│  Step 1: Concatenate K,V                     │
│    K = V = [RGB || Depth || Proprio]        │
│          = [B, 393, 768]                    │
│                                              │
│  Step 2: Compute Attention                   │
│    Q = W_q × RGB  # [B, 196, 768]           │
│    K = W_k × [RGB||Depth||Proprio]          │
│    V = W_v × [RGB||Depth||Proprio]          │
│                                              │
│    scores = Q @ K^T / √768                  │
│           = [B, 196, 393]                   │
│                                              │
│    weights = softmax(scores, dim=-1)        │
│    ↑                                         │
│    每个RGB token对所有模态token的注意力权重   │
│                                              │
│    output = weights @ V                     │
│           = [B, 196, 768]                   │
│    ↑                                         │
│    增强后的RGB特征                            │
└──────────────────────────────────────────────┘
```

**代码实现:**
```python
class AttentionFusionModel(nn.Module):
    def __init__(self, action_dim=7, hidden_dim=768):
        super().__init__()
        
        # 视觉编码器
        self.rgb_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')
        self.depth_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')
        
        # 本体感知编码器
        self.proprio_encoder = nn.Linear(10, hidden_dim)
        
        # Cross-attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # 输出层
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, rgb, depth, proprio):
        # 编码各模态
        rgb_tokens = self.rgb_encoder(pixel_values=rgb).last_hidden_state  # [B, 197, 768]
        depth_tokens = self.depth_encoder(pixel_values=depth).last_hidden_state  # [B, 197, 768]
        proprio_token = self.proprio_encoder(proprio).unsqueeze(1)  # [B, 1, 768]
        
        # 拼接K和V
        kv = torch.cat([rgb_tokens, depth_tokens, proprio_token], dim=1)  # [B, 395, 768]
        
        # Cross-attention (以RGB为query)
        # PyTorch expects (seq_len, batch, embed_dim)
        q = rgb_tokens.transpose(0, 1)  # [197, B, 768]
        kv = kv.transpose(0, 1)  # [395, B, 768]
        
        fused, attention_weights = self.cross_attention(q, kv, kv)
        fused = fused.transpose(0, 1)  # [B, 197, 768]
        
        # 池化: 取[CLS] token或平均
        fused_pooled = fused[:, 0, :]  # [B, 768] (CLS token)
        
        # 预测动作
        action = self.action_head(fused_pooled)
        return action, attention_weights
```

---

### 2.4 FiLM (Feature-wise Linear Modulation)

```
┌────────────────────────────────────────────────────┐
│         FiLM: 用一个模态调制另一个模态               │
└────────────────────────────────────────────────────┘

思想: 用语言/本体感知调制视觉特征

架构:
┌─────────────┐              ┌─────────────┐
│ RGB Image   │              │  Condition  │
│ [B,3,H,W]   │              │  (Language/ │
│             │              │  Proprio)   │
└──────┬──────┘              └──────┬──────┘
       │                            │
       ▼                            ▼
┌──────────────┐          ┌────────────────┐
│  CNN Layers  │          │  FC / LSTM     │
│              │          │                │
│  Conv1       │          │  → γ (scale)   │
│    ↓         │          │  → β (shift)   │
│  [B,C,H,W]   │          │                │
└──────┬───────┘          └────────┬───────┘
       │                           │
       │      ┌────────────────────┘
       │      │
       ▼      ▼
┌──────────────────────────────────┐
│       FiLM Layer                 │
│                                  │
│   f_out = γ ⊙ f_in + β           │
│            ↑        ↑            │
│         逐通道缩放  逐通道偏移    │
│                                  │
│   γ: [B, C, 1, 1]                │
│   β: [B, C, 1, 1]                │
│   f_in: [B, C, H, W]             │
└──────────────┬───────────────────┘
               ▼
        继续后续CNN层

示例: 语言条件化视觉处理
┌──────────────────────────────────┐
│  Instruction: "Pick red object"  │
│       ↓                          │
│  LSTM Encoder                    │
│       ↓                          │
│  [hidden_state]                  │
│       ↓                          │
│  FC → (γ_1, β_1) 用于Conv1       │
│       (γ_2, β_2) 用于Conv2       │
│       ...                        │
│                                  │
│  Image → Conv1 → FiLM(γ_1,β_1)  │
│               → Conv2 → FiLM(γ_2,β_2) │
│               → ...              │
│               → Action           │
└──────────────────────────────────┘

优点:
  ✅ 参数高效 (只学习γ和β)
  ✅ 条件信号直接影响特征
  ✅ 易于插入现有架构

应用场景:
  - 语言引导的机器人操作
  - 任务条件化的控制
  - 多任务学习
```

**代码实现:**
```python
class FiLMLayer(nn.Module):
    def __init__(self, num_channels):
        super().__init__()
        self.num_channels = num_channels
    
    def forward(self, x, gamma, beta):
        """
        x: [B, C, H, W] - 视觉特征
        gamma: [B, C] - 缩放因子
        beta: [B, C] - 偏移因子
        """
        # 扩展维度以匹配空间维度
        gamma = gamma.unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
        beta = beta.unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
        
        return gamma * x + beta

class FiLMResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, condition_dim):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
        # FiLM生成器
        self.film_gen_1 = nn.Linear(condition_dim, 2 * out_channels)
        self.film_gen_2 = nn.Linear(condition_dim, 2 * out_channels)
        
        self.film_layer_1 = FiLMLayer(out_channels)
        self.film_layer_2 = FiLMLayer(out_channels)
        
    def forward(self, x, condition):
        # 生成FiLM参数
        film_params_1 = self.film_gen_1(condition)
        gamma_1, beta_1 = film_params_1.chunk(2, dim=1)
        
        film_params_2 = self.film_gen_2(condition)
        gamma_2, beta_2 = film_params_2.chunk(2, dim=1)
        
        # 前向传播
        out = self.conv1(x)
        out = self.film_layer_1(out, gamma_1, beta_1)
        out = F.relu(out)
        
        out = self.conv2(out)
        out = self.film_layer_2(out, gamma_2, beta_2)
        out = F.relu(out)
        
        return out + x  # 残差连接

class FiLMConditionedModel(nn.Module):
    def __init__(self, action_dim=7):
        super().__init__()
        
        # 条件编码器 (语言或本体感知)
        self.condition_encoder = nn.LSTM(
            input_size=300,  # 词嵌入维度
            hidden_size=128,
            batch_first=True
        )
        
        # 视觉backbone (with FiLM)
        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)
        self.res_block1 = FiLMResBlock(64, 128, 128)
        self.res_block2 = FiLMResBlock(128, 256, 128)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, action_dim)
    
    def forward(self, image, language_tokens):
        # 编码条件 (语言)
        _, (condition, _) = self.condition_encoder(language_tokens)
        condition = condition.squeeze(0)  # [B, 128]
        
        # 视觉处理
        x = self.conv1(image)
        x = self.res_block1(x, condition)
        x = self.res_block2(x, condition)
        
        x = self.avgpool(x).squeeze(-1).squeeze(-1)
        action = self.fc(x)
        
        return action
```

---

## 3. 触觉与视觉融合

### 3.1 触觉数据的特殊性

```
┌────────────────────────────────────────────────────┐
│           触觉传感器数据处理                         │
└────────────────────────────────────────────────────┘

触觉数据类型:
1. 力/力矩传感器 (F/T Sensor)
   ┌──────────────────────┐
   │  Output: [Fx, Fy, Fz,│
   │          Tx, Ty, Tz] │
   │  维度: 6             │
   │  频率: 100-1000Hz    │
   └──────────────────────┘

2. 触觉阵列 (Tactile Array)
   ┌──────────────────────┐
   │  Output: [H, W]      │
   │  例: [16, 16]压力图   │
   │  类似"触觉图像"       │
   └──────────────────────┘

3. GelSight (基于视觉的触觉)
   ┌──────────────────────┐
   │  Output: RGB图像     │
   │  编码: 表面几何形变   │
   │  维度: [3, H, W]     │
   └──────────────────────┘

预处理:
┌──────────────────────────────────────┐
│  1. 归一化                            │
│     f_norm = (f - mean) / std        │
│                                      │
│  2. 滤波 (去除高频噪声)               │
│     f_filtered = lowpass_filter(f)   │
│                                      │
│  3. 时间窗口                          │
│     f_window = [f_t-4, ..., f_t]    │
│     (使用历史信息)                    │
└──────────────────────────────────────┘
```

### 3.2 视触融合架构

```
┌────────────────────────────────────────────────────┐
│        Vision-Tactile Fusion for Grasping          │
└────────────────────────────────────────────────────┘

场景: 机器人抓取未知物体

Stage 1: 视觉引导 (Pre-contact)
┌─────────────┐
│ RGB Image   │ → CNN → "在哪抓?"
│             │         (位置+角度)
└─────────────┘

Stage 2: 触觉调整 (In-contact)
┌─────────────┐
│ Tactile     │ → MLP → "力度够吗?"
│ Force [6]   │         (调整握力)
└─────────────┘

融合架构:
┌─────────────┐         ┌─────────────┐
│ Vision      │         │  Tactile    │
│ Encoder     │         │  Encoder    │
│ (ResNet)    │         │  (MLP)      │
│             │         │             │
│ [3,224,224] │         │    [6]      │
│      ↓      │         │     ↓       │
│   [512]     │         │   [64]      │
└──────┬──────┘         └──────┬──────┘
       │                       │
       └───────────┬───────────┘
                   ▼
        ┌────────────────────────┐
        │   Gated Fusion         │
        │                        │
        │  gate = σ(W·tactile)   │
        │  output = gate·vision  │
        │         + (1-gate)·t   │
        │                        │
        │  直觉: 接触前靠视觉,   │
        │       接触后靠触觉     │
        └────────┬───────────────┘
                 ▼
        ┌────────────────────────┐
        │  Action: [grip_force,  │
        │          adjustment]   │
        └────────────────────────┘
```

**代码实现:**
```python
class VisionTactileFusion(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 视觉编码器
        self.vision_encoder = torchvision.models.resnet18(pretrained=True)
        self.vision_encoder.fc = nn.Linear(512, 256)
        
        # 触觉编码器
        self.tactile_encoder = nn.Sequential(
            nn.Linear(6, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU()
        )
        
        # 门控融合
        self.gate_network = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # 动作预测
        self.action_head = nn.Sequential(
            nn.Linear(256 + 64, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # [grip_force, adjustment]
        )
    
    def forward(self, image, tactile):
        # 编码
        vision_feat = self.vision_encoder(image)  # [B, 256]
        tactile_feat = self.tactile_encoder(tactile)  # [B, 64]
        
        # 门控
        gate = self.gate_network(tactile_feat)  # [B, 1]
        
        # 加权融合
        fused_vision = gate * vision_feat  # 视觉权重
        fused_tactile = (1 - gate) * tactile_feat  # 触觉权重
        
        # 拼接
        fused = torch.cat([fused_vision, fused_tactile], dim=1)
        
        # 预测动作
        action = self.action_head(fused)
        return action, gate  # 返回gate用于可视化
```

---

## 4. 时序多模态融合

### 4.1 LSTM-based Fusion

```
┌────────────────────────────────────────────────────┐
│        Temporal Multi-Modal Fusion with LSTM        │
└────────────────────────────────────────────────────┘

处理时序数据: 历史观测 → 当前决策

┌──────────────────────────────────────────────┐
│  Time Steps: t-2, t-1, t                     │
│                                              │
│  每个时刻的观测:                              │
│    o_t = {RGB_t, Depth_t, Proprio_t}        │
└──────────────────────────────────────────────┘

架构:
Time t-2        Time t-1        Time t
┌───────┐       ┌───────┐       ┌───────┐
│ RGB   │       │ RGB   │       │ RGB   │
│Depth  │       │Depth  │       │Depth  │
│Proprio│       │Proprio│       │Proprio│
└───┬───┘       └───┬───┘       └───┬───┘
    │               │               │
    ▼               ▼               ▼
  Encode          Encode          Encode
    │               │               │
    ▼               ▼               ▼
┌─────────────────────────────────────┐
│          LSTM / GRU                 │
│                                     │
│   h_t = LSTM(feat_t, h_{t-1})      │
│                                     │
│   维持时序依赖关系                   │
└─────────────────┬───────────────────┘
                  ▼
        ┌───────────────────┐
        │  Action Head      │
        │  a_t = FC(h_t)    │
        └───────────────────┘

优点: 捕捉时序动态,处理部分可观测性
```

**代码实现:**
```python
class TemporalFusionModel(nn.Module):
    def __init__(self, action_dim=7):
        super().__init__()
        
        # 单帧编码器 (处理每个时刻的多模态输入)
        self.frame_encoder = LateFusionModel()  # 复用前面的模型
        self.frame_encoder.action_head = nn.Identity()  # 去掉最后的action head
        
        # LSTM处理时序
        self.lstm = nn.LSTM(
            input_size=256,  # frame_encoder输出维度
            hidden_size=256,
            num_layers=2,
            batch_first=True
        )
        
        # 动作预测
        self.action_head = nn.Linear(256, action_dim)
    
    def forward(self, rgb_seq, depth_seq, proprio_seq):
        """
        rgb_seq: [B, T, 3, 224, 224]
        depth_seq: [B, T, 1, 224, 224]
        proprio_seq: [B, T, 10]
        """
        B, T = rgb_seq.shape[:2]
        
        # 编码每一帧
        frame_features = []
        for t in range(T):
            feat = self.frame_encoder(
                rgb_seq[:, t],
                depth_seq[:, t],
                proprio_seq[:, t]
            )  # [B, 256]
            frame_features.append(feat)
        
        frame_features = torch.stack(frame_features, dim=1)  # [B, T, 256]
        
        # LSTM处理时序
        lstm_out, (h_n, c_n) = self.lstm(frame_features)  # lstm_out: [B, T, 256]
        
        # 使用最后一个时刻的输出
        final_feat = lstm_out[:, -1, :]  # [B, 256]
        
        # 预测动作
        action = self.action_head(final_feat)
        return action
```

---

## 5. 面试重点问题

**Q1: Early Fusion vs Late Fusion vs Attention Fusion怎么选？**  
A:
- Early: 简单任务,计算受限
- Late: 模态可能缺失,需要模块化
- Attention: 复杂交互,有足够数据和算力 (推荐)

**Q2: 如何处理不同模态的频率不匹配？**  
A:
- 上采样低频模态 (重复填充)
- 下采样高频模态 (平均池化)
- 异步融合 (每个模态独立时间戳)

**Q3: 触觉数据噪声大怎么办？**  
A:
- 卡尔曼滤波/低通滤波
- 使用滑动窗口平均
- 数据驱动去噪 (自编码器)

**Q4: 如何可视化多模态融合的效果？**  
A:
- 注意力权重可视化
- 梯度CAM (哪些模态贡献大)
- Ablation study (去掉某模态看性能下降)

**Q5: FiLM为什么有效？**  
A: 通过仿射变换直接调制特征,让条件信号全局影响特征提取,比简单concatenate更有表达力。

---

## 6. 参考资源

- 论文: "Multimodal Intelligence: Representation Learning, Information Fusion, and Applications"
- CVPR Tutorial: Multi-Modal Learning
- 代码库: MMF (Facebook), CLIP (OpenAI)

---

*文档版本: v1.0*  
*最后更新: 2025-11-07*

