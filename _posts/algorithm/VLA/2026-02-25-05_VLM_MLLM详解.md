---
layout: post
title: 05_VLM_MLLM详解
categories: [algorithm]
description: 
keywords: 
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

# VLM/MLLM 模型详解

## 1. VLM概述

**VLM (Vision-Language Model)**: 统一处理视觉和语言的多模态模型

```
┌────────────────────────────────────────────────────┐
│            VLM发展时间线                            │
└────────────────────────────────────────────────────┘

2021: CLIP, ALIGN
      ├─ 对比学习: 图文对齐
      └─ 零样本分类能力

2022: Flamingo, BLIP
      ├─ 视觉问答 (VQA)
      └─ 图像描述 (Captioning)

2023: LLaVA, GPT-4V, Gemini
      ├─ 整合LLM的推理能力
      └─ 多轮对话,复杂指令理解

2024: PaLI-X, PaLM-E
      ├─ 具身智能应用
      └─ 机器人控制整合
```

---

## 2. CLIP 架构详解

### 2.1 核心思想

**对比学习**: 拉近匹配图文对,推远不匹配对

```
┌────────────────────────────────────────────────────┐
│              CLIP Architecture                      │
└────────────────────────────────────────────────────┘

训练数据: 4亿 图文对 (从网络爬取)

┌──────────────────┐        ┌──────────────────┐
│   Image Encoder  │        │  Text Encoder    │
│                  │        │                  │
│   ViT or ResNet  │        │  Transformer     │
│                  │        │  (12 layers)     │
└────────┬─────────┘        └────────┬─────────┘
         │                           │
         ▼                           ▼
    Image Embedding            Text Embedding
      [B, D]                      [B, D]
         │                           │
         └───────────┬───────────────┘
                     ▼
        ┌────────────────────────────┐
        │  Cosine Similarity Matrix  │
        │                            │
        │     Text 1  Text 2  ...    │
        │  Img1  s11    s12          │
        │  Img2  s21    s22          │
        │  ...                       │
        │                            │
        │  对角线: 正样本对 (高分)    │
        │  非对角线: 负样本对 (低分) │
        └────────────┬───────────────┘
                     ▼
        ┌────────────────────────────┐
        │  Contrastive Loss          │
        │                            │
        │  L_img = -log(exp(s_ii) /  │
        │          Σ_j exp(s_ij))    │
        │                            │
        │  L_text = -log(exp(s_ii) / │
        │           Σ_j exp(s_ji))   │
        │                            │
        │  L_total = (L_img+L_text)/2│
        └────────────────────────────┘

Image Encoder (ViT-L/14):
┌──────────────────────────────────────┐
│  Input: [224, 224, 3]                │
│    ↓                                 │
│  Patch Embed: 14x14 patches          │
│    → [256, 1024] tokens              │
│    ↓                                 │
│  Transformer (24 layers)             │
│    ↓                                 │
│  [CLS] token: [1024]                 │
│    ↓                                 │
│  Projection: [1024] → [512]          │
│    ↓                                 │
│  L2 Normalize                        │
└──────────────────────────────────────┘

Text Encoder:
┌──────────────────────────────────────┐
│  Input: "A dog on the grass"         │
│    ↓                                 │
│  Tokenize: [BOS, a, dog, on, ...]    │
│    ↓                                 │
│  Token Embed + Pos Embed             │
│    ↓                                 │
│  Transformer (12 layers)             │
│    ↓                                 │
│  [EOS] token: [512]                  │
│    ↓                                 │
│  L2 Normalize                        │
└──────────────────────────────────────┘
```

### 2.2 零样本分类

```
┌────────────────────────────────────────────────────┐
│        Zero-Shot Classification with CLIP          │
└────────────────────────────────────────────────────┘

任务: 分类图像 (不需要训练)

步骤:
1. 构造文本prompt
   ┌──────────────────────────┐
   │  Class: "cat", "dog"     │
   │      ↓                   │
   │  Prompt: "A photo of a   │
   │          cat/dog"        │
   └──────────────────────────┘

2. 编码文本和图像
   ┌──────────────────────────┐
   │  img_emb = Enc_img(img)  │
   │  txt_emb = Enc_txt(text) │
   └──────────────────────────┘

3. 计算相似度
   ┌──────────────────────────┐
   │  score_cat = cosine(     │
   │    img_emb, txt_emb_cat) │
   │  score_dog = cosine(     │
   │    img_emb, txt_emb_dog) │
   └──────────────────────────┘

4. Softmax分类
   ┌──────────────────────────┐
   │  prob = softmax(         │
   │    [score_cat, score_dog]│
   │    / temperature)        │
   └──────────────────────────┘
```

**代码实现:**
```python
import torch
import clip
from PIL import Image

# 加载模型
model, preprocess = clip.load("ViT-B/32", device="cuda")

# 准备图像
image = preprocess(Image.open("dog.jpg")).unsqueeze(0).to("cuda")

# 准备文本
text_inputs = torch.cat([
    clip.tokenize("a photo of a cat"),
    clip.tokenize("a photo of a dog"),
    clip.tokenize("a photo of a bird")
]).to("cuda")

# 前向传播
with torch.no_grad():
    image_features = model.encode_image(image)  # [1, 512]
    text_features = model.encode_text(text_inputs)  # [3, 512]
    
    # 归一化
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # 计算相似度
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    
print("Label probs:", similarity)
# 输出: [[0.01, 0.97, 0.02]] → "dog"
```

---

## 3. LLaVA 架构详解

### 3.1 核心设计

**LLaVA = CLIP Vision Encoder + LLM (Vicuna)**

```
┌────────────────────────────────────────────────────┐
│              LLaVA Architecture                     │
└────────────────────────────────────────────────────┘

目标: 让LLM"看懂"图像并对话

┌─────────────┐
│   Image     │
│ [3, 336,336]│
└──────┬──────┘
       │
       ▼
┌──────────────────────────────────┐
│   CLIP Vision Encoder            │
│   (ViT-L/14, frozen)             │
│                                  │
│   Output: [576, 1024]            │
│   (576 = 24x24 patches)          │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│   Projection Layer (Trainable)   │
│                                  │
│   Linear: [1024] → [4096]        │
│   (映射到LLM的hidden维度)         │
│                                  │
│   Output: [576, 4096]            │
└──────────────┬───────────────────┘
               │
               │  Visual Tokens
               ▼
┌──────────────────────────────────┐
│        Language Model (Vicuna)   │
│                                  │
│  输入组合:                        │
│  [<image_tokens> + <text_tokens>]│
│                                  │
│  例如:                            │
│  [IMG_START]                     │
│  [576个visual tokens]            │
│  [IMG_END]                       │
│  "What is in the image?"         │
│  [/INST]                         │
│                                  │
│  ↓ Transformer Layers (32)       │
│                                  │
│  输出: 自回归生成回答             │
│  "There is a dog playing..."     │
└──────────────────────────────────┘

训练策略:
┌──────────────────────────────────┐
│  Stage 1: Pre-training           │
│    - 冻结: CLIP + LLM            │
│    - 训练: Projection Layer      │
│    - 数据: 图像-描述对 (CC3M)     │
│    - 目标: 对齐视觉-语言空间      │
│                                  │
│  Stage 2: Instruction Tuning     │
│    - 冻结: CLIP                  │
│    - 训练: Projection + LLM      │
│    - 数据: 指令数据 (LLaVA-150K) │
│    - 目标: 多轮对话能力          │
└──────────────────────────────────┘
```

### 3.2 指令数据构造

```
┌────────────────────────────────────────────────────┐
│         LLaVA Instruction Data Format               │
└────────────────────────────────────────────────────┘

数据格式:
{
  "id": "sample_1",
  "image": "path/to/image.jpg",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nWhat is the person doing in the image?"
    },
    {
      "from": "gpt",
      "value": "The person is riding a bicycle in the park."
    },
    {
      "from": "human",
      "value": "What color is the bicycle?"
    },
    {
      "from": "gpt",
      "value": "The bicycle is red."
    }
  ]
}

数据来源 (自动生成):
┌──────────────────────────────────┐
│  1. 图像 (COCO等数据集)          │
│       ↓                          │
│  2. GPT-4生成问题                │
│     "Describe this image"        │
│       ↓                          │
│  3. GPT-4生成答案                │
│     基于图像描述                  │
│       ↓                          │
│  4. 人工验证 (部分)               │
└──────────────────────────────────┘

任务类型:
  - 图像描述 (Captioning)
  - 视觉问答 (VQA)
  - 推理 (Reasoning)
  - OCR
  - 对话 (Multi-turn)
```

**代码实现:**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class LLaVAModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 视觉编码器 (CLIP)
        self.vision_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14")
        self.vision_encoder.requires_grad_(False)  # 冻结
        
        # 投影层
        self.mm_projector = nn.Linear(1024, 4096)  # CLIP → LLaMA dim
        
        # 语言模型
        self.llm = AutoModelForCausalLM.from_pretrained("lmsys/vicuna-7b-v1.5")
        
        # 特殊token
        self.img_start_token = "<im_start>"
        self.img_end_token = "<im_end>"
    
    def encode_image(self, images):
        # images: [B, 3, 336, 336]
        vision_outputs = self.vision_encoder(images)
        image_features = vision_outputs.last_hidden_state  # [B, 576, 1024]
        
        # 投影到LLM空间
        image_features = self.mm_projector(image_features)  # [B, 576, 4096]
        
        return image_features
    
    def forward(self, images, input_ids, labels=None):
        # 编码图像
        image_features = self.encode_image(images)  # [B, 576, 4096]
        
        # 获取文本嵌入
        text_embeds = self.llm.get_input_embeddings()(input_ids)  # [B, L, 4096]
        
        # 找到<image>token的位置,替换为视觉特征
        # (实际实现中需要处理拼接逻辑)
        image_token_mask = (input_ids == self.IMAGE_TOKEN_ID)
        combined_embeds = text_embeds.clone()
        combined_embeds[image_token_mask] = image_features.reshape(-1, 4096)
        
        # LLM前向传播
        outputs = self.llm(inputs_embeds=combined_embeds, labels=labels)
        
        return outputs

# 推理示例
def inference(model, image, question):
    # 预处理图像
    image_tensor = preprocess(image).unsqueeze(0).cuda()
    
    # 构造prompt
    prompt = f"{model.img_start_token}{model.img_end_token}\n{question}"
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.cuda()
    
    # 生成回答
    with torch.no_grad():
        outputs = model.generate(
            images=image_tensor,
            input_ids=input_ids,
            max_new_tokens=128,
            temperature=0.7
        )
    
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer
```

---

## 4. BLIP-2 架构详解

### 4.1 Q-Former设计

**核心创新**: 使用Q-Former桥接冻结的视觉和语言模型

```
┌────────────────────────────────────────────────────┐
│              BLIP-2 Architecture                    │
└────────────────────────────────────────────────────┘

设计哲学: 
  - 视觉编码器 (ViT) - 冻结
  - 语言模型 (OPT/Flan-T5) - 冻结
  - Q-Former - 训练 (轻量级)

┌─────────────┐
│   Image     │
└──────┬──────┘
       │
       ▼
┌──────────────────────────────────┐
│   Frozen Vision Encoder          │
│   (ViT-g)                        │
│                                  │
│   Output: [257, 1408]            │
│   (257 = CLS + 256 patches)      │
└──────────────┬───────────────────┘
               │
               │  Image Features
               ▼
┌──────────────────────────────────┐
│           Q-Former               │
│  (12-layer BERT-like)            │
│                                  │
│  组成部分:                        │
│  ┌────────────────────┐          │
│  │ Learnable Queries  │          │
│  │ [32, 768]          │          │
│  │ (32个query向量)     │          │
│  └────────────────────┘          │
│           ↓                      │
│  ┌────────────────────┐          │
│  │ Cross-Attention    │          │
│  │ Q: Queries         │          │
│  │ K,V: Image Features│          │
│  │                    │          │
│  │ "询问"图像信息      │          │
│  └────────────────────┘          │
│           ↓                      │
│  ┌────────────────────┐          │
│  │ Self-Attention     │          │
│  │ (Queries之间交互)   │          │
│  └────────────────────┘          │
│                                  │
│  Output: [32, 768]               │
│  (32个语义丰富的tokens)           │
└──────────────┬───────────────────┘
               │
               │  Visual Queries
               ▼
┌──────────────────────────────────┐
│   Linear Projection              │
│   [768] → [LLM_dim]              │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│   Frozen Language Model          │
│   (OPT-6.7B / Flan-T5-XL)        │
│                                  │
│  输入: [visual_tokens + text]    │
│  输出: 生成的文本                 │
└──────────────────────────────────┘

Q-Former训练 (两阶段):
┌──────────────────────────────────┐
│  Stage 1: Vision-Language        │
│           Representation Learning│
│                                  │
│  3个预训练任务:                   │
│  1. ITC (Image-Text Contrastive) │
│     对比学习                      │
│  2. ITG (Image-grounded Text Gen)│
│     看图说话                      │
│  3. ITM (Image-Text Matching)    │
│     匹配判断                      │
│                                  │
│  数据: 129M图文对                 │
│  时间: 数天 (16个A100)            │
│                                  │
│  Stage 2: Vision-to-Language     │
│           Generative Learning    │
│                                  │
│  任务: 连接到LLM,生成能力         │
│  训练: 只训练Q-Former和Projection │
│  数据: 同Stage 1                  │
└──────────────────────────────────┘
```

### 4.2 Q-Former详细机制

```
┌────────────────────────────────────────────────────┐
│          Q-Former Internal Structure                │
└────────────────────────────────────────────────────┘

输入:
  - Learnable Queries: [32, 768]
  - Image Features: [257, 1408]

Layer结构 (重复12次):
┌──────────────────────────────────────┐
│  1. Self-Attention (Queries之间)     │
│     Q = K = V = Queries              │
│     Output = Self-Attn(Queries)      │
│         ↓                            │
│  2. Cross-Attention (Query←Image)    │
│     Q = Queries                      │
│     K = V = Image Features           │
│     Output = Cross-Attn(Q, Img)      │
│         ↓                            │
│  3. Feed-Forward Network             │
│     Output = FFN(Output)             │
│         ↓                            │
│  更新Queries                         │
└──────────────────────────────────────┘

为什么需要Q-Former?
┌──────────────────────────────────────┐
│  问题: 直接用所有image tokens(257个) │
│       输入LLM太长,效率低             │
│                                      │
│  解决: Q-Former压缩信息              │
│       257 tokens → 32 tokens         │
│       提取最重要的视觉信息            │
│                                      │
│  类比: 人类看图 → 提取关键点 → 描述  │
└──────────────────────────────────────┘
```

**代码实现:**
```python
class QFormer(nn.Module):
    def __init__(self, num_queries=32, hidden_size=768):
        super().__init__()
        
        # 可学习的query vectors
        self.queries = nn.Parameter(torch.randn(num_queries, hidden_size))
        
        # 12层Transformer
        self.layers = nn.ModuleList([
            QFormerLayer(hidden_size) for _ in range(12)
        ])
    
    def forward(self, image_features):
        # image_features: [B, 257, 1408]
        B = image_features.size(0)
        
        # 扩展queries到batch
        queries = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, 32, 768]
        
        # 逐层处理
        for layer in self.layers:
            queries = layer(queries, image_features)
        
        return queries  # [B, 32, 768]

class QFormerLayer(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads=8)
        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads=8)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        self.norm3 = nn.LayerNorm(hidden_size)
    
    def forward(self, queries, image_features):
        # Self-attention on queries
        q = queries.transpose(0, 1)  # [32, B, 768]
        attn_out, _ = self.self_attn(q, q, q)
        queries = self.norm1(queries + attn_out.transpose(0, 1))
        
        # Cross-attention to image
        q = queries.transpose(0, 1)
        kv = image_features.transpose(0, 1)  # [257, B, 1408]
        # 需要先投影到相同维度
        attn_out, _ = self.cross_attn(q, kv, kv)
        queries = self.norm2(queries + attn_out.transpose(0, 1))
        
        # FFN
        ffn_out = self.ffn(queries)
        queries = self.norm3(queries + ffn_out)
        
        return queries
```

---

## 5. 微调技术 (Fine-tuning Methods)

### 5.1 全参数微调 vs 参数高效微调

```
┌────────────────────────────────────────────────────┐
│        Fine-tuning Methods Comparison               │
└────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────┐
│  Full Fine-tuning (全参数微调)                        │
├──────────────────────────────────────────────────────┤
│  更新: 所有参数                                       │
│  优点: 性能最好                                       │
│  缺点: - 显存需求大 (13B模型需 >100GB)               │
│       - 训练慢                                       │
│       - 易灾难性遗忘                                  │
│  适用: 数据充足 + 算力充足                            │
└──────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────┐
│  Parameter-Efficient Fine-Tuning (PEFT)              │
├──────────────────────────────────────────────────────┤
│  更新: <1%参数                                        │
│  优点: - 显存低 (13B模型 <20GB)                      │
│       - 训练快                                       │
│       - 防止遗忘                                     │
│  缺点: 性能略低于全参数 (通常<2%)                     │
│  方法: LoRA, Adapter, Prompt Tuning, Prefix Tuning   │
└──────────────────────────────────────────────────────┘

对比表:
┌─────────┬──────────┬──────────┬──────────┐
│ 方法    │训练参数量│ 显存需求 │ 性能     │
├─────────┼──────────┼──────────┼──────────┤
│ Full FT │  100%    │  ★★★★★  │ 100%     │
│ LoRA    │  0.1%    │  ★★      │ 98-99%   │
│ Adapter │  2%      │  ★★★     │ 97-98%   │
│ Prefix  │  0.01%   │  ★       │ 95-97%   │
└─────────┴──────────┴──────────┴──────────┘
```

---

## 6. 面试高频问题

**Q1: CLIP的训练需要标注吗?**  
A: 不需要。使用网络爬取的图文对 (alt-text),自然对齐,无需人工标注。

**Q2: LLaVA和BLIP-2的主要区别?**  
A:
- LLaVA: 简单projection层,需要微调LLM
- BLIP-2: Q-Former压缩信息,LLM完全冻结

**Q3: 如何评估VLM的能力?**  
A:
- VQA准确率 (VQAv2, GQA)
- 图像描述质量 (CIDEr, BLEU)
- Zero-shot分类 (ImageNet)
- 多模态推理 (NLVR2)

**Q4: VLM如何应用到机器人?**  
A:
- 语言理解: 自然语言指令 → 任务理解
- 视觉理解: 场景识别,物体检测
- 常识推理: "健康零食" → 选择水果

**Q5: 多模态预训练的数据从哪来?**  
A:
- 网络图文对: LAION-5B, CC12M
- 视频字幕: WebVid, HowTo100M
- 自动标注: 用GPT-4生成描述

---

## 7. 参考资源

### 论文
- CLIP: "Learning Transferable Visual Models From Natural Language Supervision"
- LLaVA: "Visual Instruction Tuning"
- BLIP-2: "Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"

### 代码
- Hugging Face Transformers: 统一接口
- LLaVA官方: https://github.com/haotian-liu/LLaVA
- BLIP-2官方: https://github.com/salesforce/LAVIS

---

*文档版本: v1.0*  
*最后更新: 2025-11-07*

