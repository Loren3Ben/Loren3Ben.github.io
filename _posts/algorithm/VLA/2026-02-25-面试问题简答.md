---
layout: post
title: 面试问题简答
categories: [algorithm]
description: 
keywords: 
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

# VLA


# 强化学习

**Q1: DQN的两个关键技巧是什么?**  
A: (1) Experience Replay: 打破数据相关性; (2) Target Network: 稳定Q值更新

**Q2: PPO相比TRPO有什么优势?**  
A: PPO用clipping代替TRPO的复杂KL约束,更简单易实现,且性能相当

**Q3: 为什么机器人RL需要稠密奖励?**  
A: 稀疏奖励导致探索困难,稠密奖励提供梯度信息,加速学习

**Q4: 如何解决RL的样本效率问题?**  
A: (1) 模仿学习预训练; (2) Sim2Real; (3) 模型融合; (4) 离线RL

**Q5: Actor-Critic的优势是什么?**  
A: Actor负责策略,Critic负责价值估计,减少方差,加速收敛



# Sim2Real

**Q1: 域随机化的范围如何确定?**  
A: 

- 保守估计物理参数的可能范围
- 使用ADR自动调整
- 真实测量建立边界 (系统辨识)

**Q2: Sim2Real失败的常见原因?**  
A:

- 域随机化不足 (特别是接触物理)
- 传感器噪声建模不准确
- 动作空间不匹配 (延迟问题)
- 安全约束在仿真中未考虑

**Q3: 如何评估Sim2Real的成功?**  
A:

- Zero-shot成功率 (无微调)
- 数据效率 (需要多少真实数据微调)
- 鲁棒性 (对环境变化的适应)

**Q4: 视觉Sim2Real vs 状态Sim2Real哪个更难?**  
A: 视觉更难,因为视觉域差异大。建议:

- 早期用状态 (关节角度)
- 逐步过渡到视觉
- 或使用深度图 (域差异小于RGB)

**Q5: 如何处理仿真中无法建模的现象?**  
A:

- 残差学习 (RL学习修正项)
- 在线适应
- 更换更准确的仿真器